{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f35d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361bc035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"./flowers\"\n",
    "classes=os.listdir(base_dir)\n",
    "num_class=len(classes)\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75352532",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose(\n",
    "    [  # Applying Augmentation\n",
    "        torchvision.transforms.Resize((32, 32)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomRotation(30),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "dataset = ImageFolder(base_dir, transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc9e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 500\n",
    "training_size = len(dataset) - validation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadbec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3817, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds_main = random_split(dataset,[training_size, validation_size])\n",
    "val_ds, test_ds  = random_split(val_ds_main,[300, 200])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ae451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae09f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  rose (2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e6813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=30, stride=1)\n",
    "        self.linear = nn.Linear(32, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "net = SimpleNet(num_classes=num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd14211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs):\n",
    "    \"\"\"\n",
    "    Train the network on the training set.\n",
    "    \"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss =0.0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images.unsqueeze(0))\n",
    "            loss = criterion(outputs, torch.tensor([labels]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### training wihth 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4840f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, train_ds, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "985af514",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'flower.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f00952",
   "metadata": {},
   "source": [
    "### Here is the code to recognize the flower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17c32788",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80fcd934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = './flower.pth'\n",
    "net = SimpleNet(num_classes=num_class)\n",
    "net.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5f39eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_cat = Image.open(\"p1.jpeg\").convert('RGB')\n",
    "img_cat_preprocessed = transformer(img_cat)\n",
    "batch_img_cat_tensor = torch.unsqueeze(img_cat_preprocessed, 0)\n",
    "net.eval()\n",
    "out = net(batch_img_cat_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5a85e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tulip 37.8961181640625\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.max(out, 1)\n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "print(classes[index[0]], percentage[index[0]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e719356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
